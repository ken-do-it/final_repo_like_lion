# """
# OpenRouter API í…ŒìŠ¤íŠ¸
# Django í”„ë¡œì íŠ¸ì™€ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰
# """
# import os
# from openai import OpenAI
# from dotenv import load_dotenv

# env_path = os.path.join(os.path.dirname(__file__), '..', '.env')
# print(f"ğŸ“ .env ê²½ë¡œ: {env_path}")
# print(f"ğŸ“ íŒŒì¼ ì¡´ì¬: {os.path.exists(env_path)}\n")

# # .env ë¡œë“œ
# load_dotenv(env_path)

# # OpenRouter í´ë¼ì´ì–¸íŠ¸ ìƒì„±
# client = OpenAI(
#     base_url="https://openrouter.ai/api/v1",
#     api_key=os.getenv('OPENROUTER_API_KEY'),
# )

# print("ğŸš€ OpenRouter API í…ŒìŠ¤íŠ¸ ì‹œì‘...\n")

# try:
#     # AI í˜¸ì¶œ
#     completion = client.chat.completions.create(
#         model="google/gemini-flash-1.5-8b",  # ë¬´ë£Œ ëª¨ë¸
#         messages=[
#             {
#                 "role": "user",
#                 "content": "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. 'ì„±ê³µ!'ì´ë¼ê³ ë§Œ ë‹µí•´ì£¼ì„¸ìš”."
#             }
#         ],
#         temperature=0.7,
#         max_tokens=100,
#     )
    
#     # ì‘ë‹µ ì¶œë ¥
#     response_text = completion.choices[0].message.content
#     print("âœ… API í˜¸ì¶œ ì„±ê³µ!")
#     print(f"ğŸ“ ì‘ë‹µ: {response_text}\n")
    
#     # í† í° ì‚¬ìš©ëŸ‰
#     print("ğŸ“Š í† í° ì‚¬ìš©ëŸ‰:")
#     print(f"  - Prompt: {completion.usage.prompt_tokens}")
#     print(f"  - Completion: {completion.usage.completion_tokens}")
#     print(f"  - Total: {completion.usage.total_tokens}\n")
    
#     print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
    
# except Exception as e:
#     print("âŒ ì—ëŸ¬ ë°œìƒ!")
#     print(f"ğŸ“ ì—ëŸ¬ ë‚´ìš©: {str(e)}\n")
#     print("ğŸ’¡ í™•ì¸ ì‚¬í•­:")
#     print("  1. .env íŒŒì¼ì— OPENROUTER_API_KEYê°€ ìˆëŠ”ì§€")
#     print("  2. API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€")
#     print("  3. ì¸í„°ë„· ì—°ê²°ì´ ë˜ëŠ”ì§€")

import os
from openai import OpenAI
from dotenv import load_dotenv
import time

# .env ë¡œë“œ
env_path = os.path.join(os.path.dirname(__file__), '..', '.env')
load_dotenv(env_path)

api_key = os.getenv('OPENROUTER_API_KEY')

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=api_key,
)

# í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ëª©ë¡
models_to_test = [
    "mistralai/devstral-2512:free",  # âœ… ì´ë¯¸ ì„±ê³µ
    "nvidia/nemotron-nano-9b-v2:free",
    "arcee-ai/trinity-mini:free",
    "xiaomi/mimo-v2-flash:free",
    "nvidia/nemotron-3-nano-30b-a3b:free",
    "nex-agi/deepseek-v3.1-nex-n1:free",
    "tngtech/tng-r1t-chimera:free",
    "kwaipilot/kat-coder-pro:free",
    "nvidia/nemotron-nano-12b-v2-vl:free",
    "openai/gpt-oss-120b:free",
]

print("ğŸš€ ì „ì²´ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n")
print("="*60)

successful_models = []
failed_models = []

test_prompt = "ì œì£¼ë„ 3ì¼ ì—¬í–‰ ì¼ì •ì„ ê°„ë‹¨íˆ ì¶”ì²œí•´ì£¼ì„¸ìš”. (1-2ë¬¸ì¥)"

for i, model in enumerate(models_to_test, 1):
    print(f"\n[{i}/{len(models_to_test)}] í…ŒìŠ¤íŠ¸: {model}")
    print("-"*60)
    
    try:
        start_time = time.time()
        
        completion = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "user",
                    "content": test_prompt
                }
            ],
            temperature=0.7,
            max_tokens=150,
        )
        
        elapsed = time.time() - start_time
        response = completion.choices[0].message.content
        tokens = completion.usage.total_tokens
        
        print(f"âœ… ì„±ê³µ! (ì‘ë‹µ ì‹œê°„: {elapsed:.1f}ì´ˆ)")
        print(f"ğŸ“ ì‘ë‹µ: {response[:100]}...")
        print(f"ğŸ“Š í† í°: {tokens}")
        
        successful_models.append({
            'model': model,
            'time': elapsed,
            'tokens': tokens,
            'response': response
        })
        
    except Exception as e:
        error_msg = str(e)[:100]
        print(f"âŒ ì‹¤íŒ¨: {error_msg}...")
        failed_models.append(model)
    
    # API ì œí•œ ë°©ì§€
    time.sleep(1)

# ê²°ê³¼ ìš”ì•½
print("\n" + "="*60)
print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
print("="*60)

if successful_models:
    print(f"\nâœ… ì„±ê³µí•œ ëª¨ë¸: {len(successful_models)}ê°œ\n")
    
    # ì‘ë‹µ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
    successful_models.sort(key=lambda x: x['time'])
    
    for i, result in enumerate(successful_models, 1):
        print(f"{i}. {result['model']}")
        print(f"   â±ï¸  ì‘ë‹µ ì‹œê°„: {result['time']:.1f}ì´ˆ")
        print(f"   ğŸ“Š í† í°: {result['tokens']}")
        print(f"   ğŸ“ ì‘ë‹µ: {result['response'][:80]}...")
        print()
    
    print("\nğŸ¯ ì¶”ì²œ ëª¨ë¸ (ë¹ ë¥¸ ìˆœ):")
    for i, result in enumerate(successful_models[:3], 1):
        print(f"{i}. {result['model']} ({result['time']:.1f}ì´ˆ)")

else:
    print("\nâŒ ì„±ê³µí•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

if failed_models:
    print(f"\nâŒ ì‹¤íŒ¨í•œ ëª¨ë¸: {len(failed_models)}ê°œ")
    for model in failed_models[:5]:
        print(f"  - {model}")
    if len(failed_models) > 5:
        print(f"  ... ì™¸ {len(failed_models) - 5}ê°œ")

print("\n" + "="*60)
print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")